# KYC-AML Task Definitions
# These configurations define tasks, their descriptions, expected outputs, and agent assignments
# Parameters can be injected using {parameter_name} syntax

# ==================== INTAKE TASKS ====================

validate_documents_task:
  description: >
    STEP 1 (MANDATORY): Call check_if_stage_should_skip_tool(
      processing_mode="{processing_mode}",
      stage_status="{stage_status}",
      stage_name="intake"
    )
    
    STEP 2: If tool returns should_skip=True, return {"skipped": true, "message": "<reason>"} and STOP.
    
    STEP 3: If tool returns should_skip=False, proceed with validation:
    
    Use batch_validate_documents_tool with file paths: {file_paths}
    
    Steps:
    1. Check if stage should be skipped based on processing_mode and stage_status
    2. If skip: Return JSON: {{"message": "Stage skipped - already successful", "skipped": true}}
    3. If execute: Call batch_validate_documents_tool - it handles validation, ID generation, and storage
    4. Return the result immediately - DO NOT convert PDFs to images yet
    5. PDF conversion will happen in classification stage if needed
    
    IMPORTANT: Just validate and store files. Return metadata only.
  
  expected_output: >
    JSON with MANDATORY FIRST FIELD:
    - skip_check_result: {should_skip: bool, reason: str, action: str} (from check_if_stage_should_skip_tool)
    
    IF skip_check_result.should_skip is true:
      - Return ONLY: {"skipped": true, "message": "<reason from skip_check_result>"}
    
    IF skip_check_result.should_skip is false:
      - validated_documents: [{document_id, original_filename, stored_path, file_size, 
        mime_type, validation_status}]
      - failed_documents: [{file_path, issues}]
      - summary: {total, valid, invalid}
  
  agent: document_intake_agent

# ==================== CLASSIFICATION TASKS ====================

classify_documents_task:
  description: >
    STEP 1 (MANDATORY): Call check_if_stage_should_skip_tool(
      processing_mode="{processing_mode}",
      stage_status="{stage_status}",
      stage_name="classification"
    )
    
    STEP 2: If tool returns should_skip=True, return {"skipped": true, "message": "<reason>"} and STOP.
    
    STEP 3: If tool returns should_skip=False, proceed with classification:
    
    Categories: identity_proof, address_proof, financial_statement, other
    
    Process:
    1. FOR EACH DOCUMENT - Check if PDF conversion is needed:
       a. Call check_pdf_conversion_needed_tool(document_id) to check if it's a PDF
       b. ONLY IF needs_conversion=true: 
          - Call convert_pdf_to_images_tool(document_id)
          - The conversion returns child_document_ids (list of image IDs created from PDF)
          - For EACH child_document_id: Call get_document_by_id_tool(child_document_id) to get its metadata
          - Use the child document's stored_path for classification
       c. IMPORTANT: Do NOT convert image files (jpg, png, etc.) - they are already images!
    
    2. FOR EACH DOCUMENT TO CLASSIFY (original images OR child images from PDF):
       a. Get the file path:
          - For original images: Use extract_document_file_path_tool(document)
          - For child images: Use stored_path from get_document_by_id_tool(child_document_id)
       b. Call make_classifier_api_request(endpoint_path="/predict", method="POST", file_path=...) to classify
       c. Parse the API response to get predicted_class and confidence
    
    3. Update metadata for EACH classified document:
       - If classifying a child image: Update the CHILD document metadata with classification results
       - Also update the PARENT PDF metadata to reference the child classification
       
       update_document_metadata_tool(
         document_id="DOC_..." (child ID for child docs, original ID for images),
         stage_name="classification",
         status="success" or "fail",
         msg="Document classified as Passport" or error description,
         additional_data={
           "document_type": "Passport",
           "confidence": 0.95,
           "categories": ["identity_proof"],
           "requires_review": false,
           "api_response": <full raw API response dict>
         }
       )
    
    Documents: {documents}
  
  expected_output: >
    JSON with MANDATORY FIRST FIELD:
    - skip_check_result: {should_skip: bool, reason: str, action: str} (from check_if_stage_should_skip_tool)
    
    IF skip_check_result.should_skip is true:
      - Return ONLY: {"skipped": true, "message": "<reason from skip_check_result>"}
    
    IF skip_check_result.should_skip is false:
      - classified_documents: [{document_id, document_type, confidence, categories, 
        requires_review}]
      - failed_documents: [{document_id, error}]
      - summary: {total, success, failed}
  
  agent: document_classifier_agent

# ==================== EXTRACTION TASKS ====================

extract_document_data_task:
  description: >
    STEP 1 (MANDATORY): Call check_if_stage_should_skip_tool(
      processing_mode="{processing_mode}",
      stage_status="{stage_status}",
      stage_name="extraction"
    )
    
    STEP 2: If tool returns should_skip=True, return {"skipped": true, "message": "<reason>"} and STOP.
    
    STEP 3: If tool returns should_skip=False, proceed with extraction:
    1. Get documents in 'classification' stage using get_documents_by_stage("classification")
    2. FOR EACH document:
       a. Call extract_text_from_image_tool(file_path=stored_path) to get Vision API response
       b. Vision API returns structured JSON: {success, text, confidence, word_count, char_count}
       c. Parse the extracted text based on document_type from classification to identify fields:
          
          For Passport/Voter ID/Driving License (identity documents):
          - Full name, Date of birth, Document number
          - Issue date, Expiry date, Issuing authority
          
          For Address proof (utility bills, bank statements):
          - Full name, Complete address
          - Document date, Issuing organization
          
          For PAN/Aadhar:
          - Name, ID number, Date of birth
       
       d. Update metadata using update_document_metadata_tool:
          update_document_metadata_tool(
            document_id="DOC_...",
            stage_name="extraction",
            status="success" or "fail",
            msg="Successfully extracted structured fields from document" or error description,
            additional_data={
              "extracted_fields": {
                "name": "John Doe",
                "document_number": "A12345678",
                "dob": "1990-01-01",
                ...
              },
              "raw_text": full_extracted_text,
              "confidence": vision_api_confidence,
              "word_count": word_count,
              "extraction_quality": calculated_quality_score,
              "fields_found": ["name", "document_number", "dob"],
              "fields_missing": ["expiry_date"],
              "vision_api_response": <full Vision API response>
            }
          )
    
    IMPORTANT:
    - Vision API provides raw text and confidence
    - Your job is to parse the text and extract structured fields based on document_type
    - Use document_type from classification to know what fields to look for
    - Mark extraction quality based on how many expected fields were found
  
  expected_output: >
    JSON with MANDATORY FIRST FIELD:
    - skip_check_result: {should_skip: bool, reason: str, action: str} (from check_if_stage_should_skip_tool)
    
    IF skip_check_result.should_skip is true:
      - Return ONLY: {"skipped": true, "message": "<reason from skip_check_result>"}
    
    IF skip_check_result.should_skip is false:
      - extracted_documents: [{document_id, document_type, extracted_fields, confidence,
        extraction_quality, fields_found, fields_missing}]
      - failed_documents: [{document_id, error}]
      - summary: {total, success, failed}
          "confidence": 0.95,
          "extraction_quality": 0.90,
          "fields_found": 5,
          "fields_missing": 0
        }
      ],
      "summary": {
        "total": 5,
        "successful": 4,
        "failed": 1,
        "average_quality": 0.88
      }
    }
  
  agent: document_extraction_agent

batch_extract_documents_task:
  description: >
    Extract data from multiple documents in batch for case {case_id}.
    
    Batch processing strategy:
    1. Group documents by type for consistent extraction
    2. Use appropriate OCR API for each document type
    3. Implement parallel processing where possible
    4. Aggregate results with quality metrics
    5. Generate extraction summary report
    
    Documents to process: {documents}
    Priority: {priority}
  
  expected_output: >
    JSON object containing:
    - extractions: List of individual extraction results (same format as extract_document_data_task)
    - summary:
      - total_documents: Count of documents processed
      - successful_extractions: Count of successful extractions
      - failed_extractions: Count of failed extractions
      - average_quality_score: Mean extraction quality across all documents
      - total_processing_time: Time taken for batch
      - case_id: Case identifier
  
  agent: document_extraction_agent

# ==================== WORKFLOW COORDINATION TASKS ====================

orchestrate_workflow_task:
  description: >
    Orchestrate the complete document processing workflow for case {case_id}.
    
    Workflow stages:
    1. Document intake and validation
    2. Document classification
    3. Data extraction
    4. Quality validation and review
    5. Result aggregation and reporting
    
    Coordination responsibilities:
    - Monitor progress of each stage
    - Handle errors and retries
    - Maintain workflow state
    - Coordinate between specialized agents
    - Make decisions about workflow progression
    - Escalate issues requiring human intervention
    
    Input documents: {file_paths}
  
  expected_output: >
    JSON object containing:
    - case_id: Case identifier
    - workflow_status: "completed", "partial", or "failed"
    - stages_completed: List of completed workflow stages
    - intake_results: Results from intake stage
    - classification_results: Results from classification stage
    - extraction_results: Results from extraction stage
    - quality_metrics:
      - overall_quality_score: 0.0-1.0
      - documents_requiring_review: Count
      - processing_time: Total time in seconds
    - errors: List of any errors encountered
    - recommendations: Suggested next steps or actions
  
  agent: supervisor_agent

validate_and_report_task:
  description: >
    Validate all processing results for case {case_id} and generate comprehensive report.
    
    Validation checks:
    1. Verify all documents were processed
    2. Check data completeness for each document
    3. Validate field consistency across documents
    4. Identify any data quality issues
    5. Check compliance with KYC/AML requirements
    
    Report generation:
    1. Create summary of all processed documents
    2. Highlight documents requiring review
    3. List extracted key information
    4. Include quality metrics
    5. Provide recommendations for case progression
  
  expected_output: >
    Comprehensive JSON report with:
    - case_summary: Overview of case processing
    - document_inventory: List of all documents with status
    - extracted_information: Consolidated data from all documents
    - quality_assessment: Quality metrics and scores
    - compliance_checklist: KYC/AML requirement coverage
    - review_required: List of items needing human review
    - recommendations: Next steps for case handling
    - report_timestamp: ISO 8601 timestamp
  
  agent: supervisor_agent
