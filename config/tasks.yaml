# KYC-AML Task Definitions
# These configurations define tasks, their descriptions, expected outputs, and agent assignments
# Parameters can be injected using {parameter_name} syntax

# ==================== INTAKE TASKS ====================

validate_documents_task:
  description: >
    STEP 1 (MANDATORY): Call check_if_stage_should_skip_tool(
      processing_mode="{processing_mode}",
      stage_status="{stage_status}",
      stage_name="intake"
    )
    
    STEP 2: If tool returns should_skip=True, return {"skipped": true, "message": "<reason>"} and STOP.
    
    STEP 3: If tool returns should_skip=False, resolve file/directory inputs:
    
    Call resolve_document_paths_tool with inputs:
      - paths: {file_paths}
      - recursive: false
    
    STEP 4: Validate and store all resolved files:
    - Use batch_validate_documents_tool with resolved_files from STEP 3
    - This tool handles validation, ID generation, and metadata creation
    
    STEP 5: If any validated documents are PDFs, convert them to child image docs:
    - For each validated document where metadata.extension == ".pdf":
      * Call convert_pdf_to_images_tool(document_id)
      * Capture child document IDs and parent document IDs
    
    STEP 6: Queue documents for classification:
    - Call queue_documents_for_classification_tool with:
      * document_paths: list of stored_path for validated documents
      * child_documents: list of {child_id, parent_id}
      * require_confirmation: {require_queue_confirmation}
    
    STEP 7 (OPTIONAL - SMART LINKING): 
    - Check if you have a case_id in your context/inputs
    - ONLY IF case_id exists and is not empty:
      * For each validated document_id, call link_document_to_case_tool(document_id, case_id)
      * This ensures documents are immediately associated with the case
    - If case_id is NOT provided or empty, skip this step - documents remain independent
    
    IMPORTANT: Documents are processed independently. case_id is OPTIONAL. 
    Linking happens only when case context is explicitly provided.
  
  expected_output: >
    JSON with MANDATORY FIRST FIELD:
    - skip_check_result: {should_skip: bool, reason: str, action: str} (from check_if_stage_should_skip_tool)
    
    IF skip_check_result.should_skip is true:
      - Return ONLY: {"skipped": true, "message": "<reason from skip_check_result>"}
    
    IF skip_check_result.should_skip is false:
      - resolved_paths: {total_resolved, directories_scanned, skipped_paths}
      - validated_documents: [{document_id, original_filename, stored_path, file_size, 
        mime_type, validation_status}]
      - failed_documents: [{file_path, issues}]
      - pdf_conversions: [{document_id, child_documents, total_pages}]
      - queue_summary: {documents_queued, child_documents_queued, documents_skipped, child_documents_skipped}
      - summary: {total, valid, invalid}
  
  agent: document_intake_agent

# ==================== CLASSIFICATION TASKS ====================

classify_documents_task:
  description: >
    INPUT CONTEXT:
    - document_metadata: {document_metadata} (FULL metadata object with all status blocks)
    - document_id: {document_id} (also available as metadata['document_id'])

    STEP 1 (MANDATORY): Call check_if_stage_should_skip_tool(
      processing_mode="{processing_mode}",
      stage_status="{stage_status}",
      stage_name="classification"
    )
    
    STEP 2: If tool returns should_skip=True, return {"skipped": true, "message": "<reason>"} and STOP.
    
    STEP 3: If tool returns should_skip=False, proceed with classification:
    
    Categories: identity_proof, address_proof, financial_statement, other
    
    Process:
    1. Document metadata is PROVIDED in inputs - use it directly (no need to fetch)
    2. Extract document_id from metadata['document_id'] or use provided document_id
    3. Log/announce: "Processing document: " followed by the document_id value
    4. Use metadata['stored_path'] for file location OR call extract_document_file_path_tool(document_metadata)
    5. Use file path from metadata to classify
    6. Call make_classifier_api_request(file_path=...) to classify the document
    
    8. Update metadata after processing:
       
       update_document_metadata_tool(
         document_id="DOC_..." (child ID for child docs, original ID for images),
         stage_name="classification",
         status="success" or "fail",
         msg="Document classified as Passport" or error description,
         additional_data={
           "document_type": "Passport",
           "confidence": 0.95,
           "categories": ["identity_proof"],
           "requires_review": false,
           "api_response": <full raw API response dict>
         }
       )
    
    IMPORTANT: Document metadata is provided in inputs - use it directly. 
    The metadata includes all status blocks (intake, classification, extraction).
    Only call get_document_by_id_tool if you need to refresh metadata for specific reasons.
  
  expected_output: >
    JSON with MANDATORY FIRST FIELD:
    - skip_check_result: {should_skip: bool, reason: str, action: str} (from check_if_stage_should_skip_tool)
    
    IF skip_check_result.should_skip is true:
      - Return ONLY: {"skipped": true, "message": "<reason from skip_check_result>"}
    
    IF skip_check_result.should_skip is false:
      - classified_documents: [{document_id, document_type, confidence, categories, 
        requires_review}]
      - failed_documents: [{document_id, error}]
      - summary: {total, success, failed}
  
  agent: document_classifier_agent

# ==================== EXTRACTION TASKS ====================

extract_document_data_task:
  description: >
    INPUT CONTEXT:
    - document_metadata: {document_metadata} (FULL metadata WITH classification results)
    - document_id: {document_id} (also available as metadata['document_id'])
    
    IMPORTANT: Metadata includes classification block with document_type - use it!

    STEP 1 (MANDATORY): Call check_if_stage_should_skip_tool(
      processing_mode="{processing_mode}",
      stage_status="{stage_status}",
      stage_name="extraction"
    )
    
    STEP 2: If tool returns should_skip=True, return {"skipped": true, "message": "<reason>"} and STOP.
    
    STEP 3: If tool returns should_skip=False, proceed with extraction:
    1. Document metadata is PROVIDED in inputs with classification results
    2. Extract document_type from metadata['classification']['document_type'] - this tells you what to extract!
    3. Extract document_id from metadata['document_id']
    4. Log/announce: "Processing document: " followed by the document_id value
    5. Use metadata['stored_path'] for file location OR call extract_document_file_path_tool(document_metadata)
    6. If has_children=true (parent PDF): Mark extraction as "skipped" - children will be extracted
    7. Otherwise: Call extract_text_from_image_tool(file_path=...) for the document
    8. Vision API returns structured JSON: {success, text, confidence, word_count, char_count}
    9. Parse extracted text using document_type from metadata['classification']['document_type']:
          
    9. Parse extracted text using document_type from metadata['classification']['document_type']:
          
          For Passport/Voter ID/Driving License (identity documents):
          - Full name, Date of birth, Document number
          - Issue date, Expiry date, Issuing authority
          
          For Address proof (utility bills, bank statements):
          - Full name, Complete address
          - Document date, Issuing organization
          
          For PAN/Aadhar:
          - Name, ID number, Date of birth
       
    10. Update metadata using update_document_metadata_tool:
       update_document_metadata_tool(
         document_id="DOC_...",
            stage_name="extraction",
            status="success" or "fail",
            msg="Successfully extracted structured fields from document" or error description,
            additional_data={
              "extracted_fields": {
                "name": "John Doe",
                "document_number": "A12345678",
                "dob": "1990-01-01",
                ...
              },
              "raw_text": full_extracted_text,
              "confidence": vision_api_confidence,
              "word_count": word_count,
              "extraction_quality": calculated_quality_score,
              "fields_found": ["name", "document_number", "dob"],
              "fields_missing": ["expiry_date"],
              "vision_api_response": <full Vision API response>
            }
          )
    
    IMPORTANT:
    - Document metadata PROVIDED with classification results - use metadata['classification']['document_type']
    - Vision API provides raw text - parse it based on document_type from classification
    - No need to call get_document_by_id_tool - you have complete context in inputs
    - metadata['stored_path'] has file location
    - Mark extraction quality based on how many expected fields were found
  
  expected_output: >
    JSON with MANDATORY FIRST FIELD:
    - skip_check_result: {should_skip: bool, reason: str, action: str} (from check_if_stage_should_skip_tool)
    
    IF skip_check_result.should_skip is true:
      - Return ONLY: {"skipped": true, "message": "<reason from skip_check_result>"}
    
    IF skip_check_result.should_skip is false:
      - extracted_documents: [{document_id, document_type, extracted_fields, confidence,
        extraction_quality, fields_found, fields_missing}]
      - failed_documents: [{document_id, error}]
      - summary: {total, success, failed}
          "confidence": 0.95,
          "extraction_quality": 0.90,
          "fields_found": 5,
          "fields_missing": 0
        }
      ],
      "summary": {
        "total": 5,
        "successful": 4,
        "failed": 1,
        "average_quality": 0.88
      }
    }
  
  agent: document_extraction_agent

summarize_documents_task:
  description: >
    Summarize documents using provided metadata only.

    INPUTS:
    - document_ids: {document_ids}
    - documents_metadata: {documents_metadata}

    REQUIREMENTS:
    - Use only the provided metadata. Do NOT call processing tools.
    - Produce a concise bullet list summary.
    - Include document_id, document_type (if available), key identifiers (if present),
      status of intake/classification/extraction, and any requires_review flags.
    - If data is missing, note "unknown" rather than guessing.

  expected_output: >
    Bullet list text only. Example:
    - DOC_...: Passport, status intake=success, classification=success, extraction=success
    - DOC_...: Utility bill, requires_review=true, extraction=pending

  agent: document_summary_agent

batch_extract_documents_task:
  description: >
    Extract data from multiple documents in batch for case {case_id}.
    
    Batch processing strategy:
    1. Group documents by type for consistent extraction
    2. Use appropriate OCR API for each document type
    3. Implement parallel processing where possible
    4. Aggregate results with quality metrics
    5. Generate extraction summary report
    
    Documents to process: {documents}
    Priority: {priority}
  
  expected_output: >
    JSON object containing:
    - extractions: List of individual extraction results (same format as extract_document_data_task)
    - summary:
      - total_documents: Count of documents processed
      - successful_extractions: Count of successful extractions
      - failed_extractions: Count of failed extractions
      - average_quality_score: Mean extraction quality across all documents
      - total_processing_time: Time taken for batch
      - case_id: Case identifier
  
  agent: document_extraction_agent

# ==================== WORKFLOW COORDINATION TASKS ====================

orchestrate_workflow_task:
  description: >
    Orchestrate the complete document processing workflow for case {case_id}.
    
    Workflow stages:
    1. Document intake and validation
    2. Document classification
    3. Data extraction
    4. Quality validation and review
    5. Result aggregation and reporting
    
    Coordination responsibilities:
    - Monitor progress of each stage
    - Handle errors and retries
    - Maintain workflow state
    - Coordinate between specialized agents
    - Make decisions about workflow progression
    - Escalate issues requiring human intervention
    
    Input documents: {file_paths}
  
  expected_output: >
    JSON object containing:
    - case_id: Case identifier
    - workflow_status: "completed", "partial", or "failed"
    - stages_completed: List of completed workflow stages
    - intake_results: Results from intake stage
    - classification_results: Results from classification stage
    - extraction_results: Results from extraction stage
    - quality_metrics:
      - overall_quality_score: 0.0-1.0
      - documents_requiring_review: Count
      - processing_time: Total time in seconds
    - errors: List of any errors encountered
    - recommendations: Suggested next steps or actions
  
  agent: supervisor_agent

validate_and_report_task:
  description: >
    Validate all processing results for case {case_id} and generate comprehensive report.
    
    Validation checks:
    1. Verify all documents were processed or wait for processing to complete
    2. Check data completeness for each document
    3. Validate field consistency across documents
    4. Identify any data quality issues
    5. Check compliance with KYC/AML requirements
    
    Report generation:
    1. Create summary of all processed documents
    2. Highlight documents requiring review
    3. List extracted key information
    4. Include quality metrics
    5. Provide recommendations for case progression
  
  expected_output: >
    Comprehensive JSON report with:
    - case_summary: Overview of case processing
    - document_inventory: List of all documents with status
    - extracted_information: Consolidated data from all documents
    - quality_assessment: Quality metrics and scores
    - compliance_checklist: KYC/AML requirement coverage
    - review_required: List of items needing human review
    - recommendations: Next steps for case handling
    - report_timestamp: ISO 8601 timestamp
  
  agent: supervisor_agent
